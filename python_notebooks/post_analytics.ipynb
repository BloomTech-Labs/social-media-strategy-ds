{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "U4-S1-NLP",
      "language": "python",
      "name": "u4-s1-nlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "pep8 of post_analytics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsyzuUGWzo7e",
        "colab_type": "code",
        "outputId": "c57ba334-b805-4fb6-f86f-8e4ff2c174b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Pip install what's not in Colab\n",
        "pip install python-dotenv\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading https://files.pythonhosted.org/packages/cb/2a/07f87440444fdf2c5870a710b6770d766a1c7df9c827b0c90e807f1fb4c5/python_dotenv-0.13.0-py2.py3-none-any.whl\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-0.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyFtzguYzo7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "from dotenv import load_dotenv\n",
        "from gensim import corpora\n",
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from gensim.utils import simple_preprocess\n",
        "from spacy.tokenizer import Tokenizer\n",
        "\n",
        "import gensim\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "import tweepy\n",
        "import tweepy\n",
        "\n",
        "# Loading ...\n",
        "load_dotenv()\n",
        "\n",
        "# Find the secrets\n",
        "TWITTER_API_KEY = os.getenv(\"TWITTER_API_KEY\")\n",
        "TWITTER_API_SECRET = os.getenv(\"TWITTER_API_SECRET\")\n",
        "TWITTER_ACCESS_TOKEN = os.getenv(\"TWITTER_ACCESS_TOKEN\")\n",
        "TWITTER_ACCESS_TOKEN_SECRET = os.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\")\n",
        "\n",
        "# Enable the ability to access the Twitter API\n",
        "auth = tweepy.OAuthHandler(TWITTER_API_KEY, TWITTER_API_SECRET)\n",
        "auth.set_access_token(TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_TOKEN_SECRET)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l35EIHSJzo7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start with Lambda School as the user\n",
        "screen_name = 'LambdaSchool'\n",
        "\n",
        "def get_information(screen_name):\n",
        "  \"\"\"\n",
        "  Calls twitter and retreves tweets\n",
        "\n",
        "  Returns a DataFrame\n",
        "  \"\"\"\n",
        "    new_tweets = api.user_timeline(screen_name=screen_name, count=200,\n",
        "                                   tweet_mode='extended')\n",
        "    tweets = []\n",
        "    retweet_count = []\n",
        "    favorite_count = []\n",
        "\n",
        "    for tweet in range(len(new_tweets)):\n",
        "\n",
        "        status = new_tweets[tweet]\n",
        "\n",
        "        # convert to string\n",
        "        json_str = json.dumps(status._json)\n",
        "\n",
        "        # deserialise string into python object\n",
        "        parsed = json.loads(json_str)\n",
        "        tweets.append(parsed.get('full_text'))\n",
        "        retweet_count.append(parsed.get('retweet_count'))\n",
        "        favorite_count.append(parsed.get('retweet_count'))\n",
        "    # return a dataframe\n",
        "    return pd.DataFrame(list(zip(tweets, retweet_count, favorite_count)),\n",
        "                        columns=['tweets', 'retweet_count', 'favorite_count'])\n",
        "\n",
        "# Call function to get information\n",
        "df = get_information(screen_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqZg9UPpzo7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the user tweet post where followers most engaged with\n",
        "df = df.sort_values(by=['retweet_count', 'favorite_count'], ascending=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_0vgGWjzo7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean and see the tweets\n",
        "df['tweets_clean'] = df['tweets'].apply(lambda x: x[0:-1].replace('\\n\\n', ' '))\n",
        "df['tweets_clean']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzCYLTONzo70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def clear_emoji(text):\n",
        "  \"\"\"\n",
        "  Clean the emoji characters from the tweets\n",
        "  \"\"\"\n",
        "\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "          u\"\\U0001F600-\\U0001F64F\"\n",
        "          u\"\\U0001F300-\\U0001F5FF\"\n",
        "          u\"\\U0001F680-\\U0001F6FF\"\n",
        "          u\"\\U0001F1E0-\\U0001F1FF\" \n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "# Call clean_emoji function and apply results to new column\n",
        "df['tweets_clean'] = df['tweets_clean'].apply(clear_emoji)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYZ6yPGnzo73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Further cleaning of the tweets for use\n",
        "\n",
        "df['tweets_hashtag'] = df['tweets_clean'].apply(\n",
        "    lambda x: [col for col in x.split() if col.startswith('#')])\n",
        "\n",
        "df['tweets_hashtag'] = df['tweets_hashtag'].apply(\n",
        "    lambda x: ','.join(map(str, x)))\n",
        "\n",
        "df['tweets_mention'] = df['tweets_clean'].apply(\n",
        "    lambda x: [col for col in x.split() if col.startswith('@')])\n",
        "\n",
        "df['tweets_mention'] = df['tweets_mention'].apply(\n",
        "    lambda x: ','.join(map(str, x)))\n",
        "\n",
        "df['tweets'] = df['tweets_clean']\n",
        "\n",
        "df = df.drop('tweets_clean', axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLWR-Rz2zo8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Commence the Natural Language Processing \n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "\n",
        "STOP_WORDS = nlp.Defaults.stop_words.union([\"&amp\", \"rt\", \"lambda\", \"&amp;\",\n",
        "                                            \"i’m\", \"we're\", \"you’re\", \"it’s\",\n",
        "                                            'thanks', 'student', 'school.'])\n",
        "\n",
        "tokens = []\n",
        "\n",
        "\"\"\" Update those tokens w/o stopwords\"\"\"\n",
        "for doc in tokenizer.pipe(df['tweets'], batch_size=500):\n",
        "\n",
        "    doc_tokens = []\n",
        "\n",
        "    for token in doc:\n",
        "        if (token.text.lower() not in STOP_WORDS) &\n",
        "         (token.is_punct == False) &\n",
        "        (token.is_space == False):\n",
        "            doc_tokens.append(token.text.lower())\n",
        "\n",
        "    tokens.append(doc_tokens)\n",
        "\n",
        "df['tokens'] = tokens\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ulan7v_vzo8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id2word = corpora.Dictionary(df['tokens'])\n",
        "\n",
        "corpus = [id2word.doc2bow(text) for text in df['tokens']]\n",
        "\n",
        "lda = LdaMulticore(corpus=corpus,\n",
        "                   id2word=id2word,\n",
        "                   random_state=723812,\n",
        "                   num_topics=15,\n",
        "                   passes=10,\n",
        "                   workers=8)\n",
        "\n",
        "lda.print_topics()\n",
        "\n",
        "words = [re.findall(r'\"([^\"]*)\"', t[1]) for t in lda.print_topics()]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-uMAUXaEFoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here's the topics\n",
        "topics = [' '.join(t[0:5]) for t in words]\n",
        "\n",
        "# Print the topics\n",
        "for id, t in enumerate(topics):\n",
        "\n",
        "    print(f\"------ Topic {id} ------\")\n",
        "    print(t, end=\"\\n\\n\")\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}